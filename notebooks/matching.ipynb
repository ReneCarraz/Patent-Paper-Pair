{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patent Paper Pair Matching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and important funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pyalex\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from functools import reduce\n",
    "from operator import ior\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from json import JSONDecodeError\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from pyalex import Institutions\n",
    "\n",
    "data_folder = '/Folder/To/Your/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(iterable, n=50):\n",
    "    \"\"\"\n",
    "    Provides an output iterable returning batches of size `n` of a given input iterable.\n",
    "    \"\"\"\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx : min(ndx + n, l)]\n",
    "\n",
    "\n",
    "def convert_date(date_string, format=\"%Y-%m-%d\"):\n",
    "    \"\"\"\n",
    "    Converts a date string in the format YYYY-MM-DD to a datetime.date object.\n",
    "    \"\"\"\n",
    "    return datetime.strptime(date_string, format).date()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.openalex_db import OpenAlexLocalDB\n",
    "openalex_db = OpenAlexLocalDB(data_folder + \"start/openalex.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for long fetching loops, in case we need to re-run the same query\n",
    "requests_cache.install_cache(\n",
    "    data_folder + \"cache/openalex/openalex.sqlite\", backend=\"sqlite\", expire_after=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full gov list of patents\n",
    "patents = dd.read_parquet(data_folder + \"start/dask/patents.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaNs and duplicates\n",
    "patents = patents.dropna().sort_values(\"patent_filing_date\").drop_duplicates(\n",
    "    subset=['patent_title', 'patent_assignee'], keep='first',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only patents with assignee country JP\n",
    "patents = patents[patents[\"patent_assignee_country\"] == \"JP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['patent_filing_year'] = \\\n",
    "    patents['patent_filing_date'].str.split('-').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num patents before date filtering: {patents.shape[0]}\")\n",
    "min_year = 2004\n",
    "max_year = 2018\n",
    "patents = patents[(patents['patent_filing_year'] >= min_year) & (patents['patent_filing_year'] <= max_year)]\n",
    "f\"Num patents after date filtering: {patents.shape[0]}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by assignee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our list of 35 national research institutions to consider\n",
    "research_institutions_list = None # Provide your own list of research institutions here\n",
    "# Remove RIKEN (will be handled manually)\n",
    "research_institutions_list = [name for name in research_institutions_list if name != \"RIKEN\"]\n",
    "research_institutions = {name: name.lower() for name in research_institutions_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "universities_exclude_list = [\n",
    "    \"Polytechnic University\",\n",
    "    \"Tama University\",\n",
    "    \"Ohu University\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignees_to_exclude_list = [\n",
    "    \"Toyota Technical Institute At Chicago\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universities_df = None # Provide your own list of universities here\n",
    "print(f\"Number of universities in file: {len(universities_df)}\")\n",
    "universities_df = universities_df[~universities_df['name_en'].isin(universities_exclude_list)]\n",
    "print(f\"Number of universities after excluding: {len(universities_df)}\")\n",
    "universities = {name_en: name_en.lower() for name_en in universities_df['name_en'].tolist()}\n",
    "universities_jp_name_mapping = {name_en: name_jp for name_en, name_jp in zip(universities_df['name_en'], universities_df['name_jp'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_names = research_institutions | universities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_mask = patents[\"patent_assignee\"].str.lower().str.contains('|'.join(entity_names.values()), na=False, regex=True)\n",
    "# use exact match for RIKEN\n",
    "riken_mask = patents[\"patent_assignee\"].str.lower() == \"riken\"\n",
    "patents = patents[entity_mask | riken_mask].compute()\n",
    "\n",
    "# exclude the assignees in exclusion list\n",
    "patents = patents[~patents[\"patent_assignee\"].str.lower().isin(assignees_to_exclude_list)]\n",
    "\n",
    "f\"Num patents after assignee filtering: {patents.shape[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_type = {}\n",
    "for entity in research_institutions.keys():\n",
    "    entity_type[entity] = \"institution\"\n",
    "for entity in universities.keys():\n",
    "    entity_type[entity] = \"university\"\n",
    "entity_type[\"RIKEN\"] = \"institution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add jp entity name\n",
    "patents[\"entity_name_jp\"] = patents[\"patent_assignee\"].map(universities_jp_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 'entity_name' column\n",
    "patents[\"entity_name\"] = \"Unknown\"\n",
    "\n",
    "# for each entity, add it to the entity_name column\n",
    "for entity, entity_lower in entity_names.items():\n",
    "    entity_mask = patents[\"patent_assignee\"].str.lower().str.contains(entity_lower, na=False, regex=False)\n",
    "    if entity_mask.sum() > 0:\n",
    "        patents.loc[entity_mask, \"entity_name\"] = entity\n",
    "\n",
    "patents.loc[patents['patent_assignee'] == \"RIKEN\", \"entity_name\"] = \"RIKEN\"\n",
    "\n",
    "patents[\"entity_name\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add entity type\n",
    "patents[\"entity_type\"] = patents[\"entity_name\"].map(entity_type)\n",
    "patents.loc[patents['entity_name'] == \"RIKEN\", \"entity_type\"] = \"institution\"\n",
    "patents[\"entity_type\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce csv of number of patents per entity_name + assignee pair\n",
    "patent_counts = patents.groupby([\"entity_name\", \"patent_assignee\"]).size().reset_index(name='counts').sort_values([\"entity_name\", \"counts\"], ascending=[True, False])\n",
    "patent_counts.to_csv(data_folder + \"result/patent_counts_by_entity_assignee.csv\", index=False)\n",
    "\n",
    "# produce a universities only and a institutions only csv\n",
    "patent_counts_university = patents[patents[\"entity_type\"] == \"university\"].groupby([\"entity_name\", \"patent_assignee\"]).size().reset_index(name='counts').sort_values([\"entity_name\", \"counts\"], ascending=[True, False])\n",
    "patent_counts_university.to_csv(data_folder + \"result/patent_counts_by_university_assignee.csv\", index=False)\n",
    "patent_counts_institution = patents[patents[\"entity_type\"] == \"institution\"].groupby([\"entity_name\", \"patent_assignee\"]).size().reset_index(name='counts').sort_values([\"entity_name\", \"counts\"], ascending=[True, False])\n",
    "patent_counts_institution.to_csv(data_folder + \"result/patent_counts_by_institution_assignee.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_counts = patents.groupby(\"entity_name\").size().reset_index(name=\"num_patents\")\n",
    "entity_counts_mapping = {entity_name: num_patents for entity_name, num_patents in zip(entity_counts[\"entity_name\"], entity_counts[\"num_patents\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a list of entity patent counts, including 0\n",
    "entities_df = pd.DataFrame({\"entity_name\": list(entity_names.keys()) + [\"RIKEN\"]})\n",
    "entities_df['entity_type'] = entities_df['entity_name'].map(entity_type)\n",
    "entities_df['num_patents'] = entities_df['entity_name'].map(entity_counts_mapping).fillna(0).astype(int)\n",
    "entities_df.sort_values([\"num_patents\", \"entity_name\"], ascending=[False, True], inplace=True)\n",
    "entities_df.to_csv(data_folder + \"result/entity_patent_counts.csv\", index=False)\n",
    "\n",
    "# also produce one with only university names, and one with only institution names\n",
    "entities_df[entities_df[\"entity_type\"] == \"university\"].to_csv(data_folder + \"result/university_patent_counts.csv\", index=False)\n",
    "entities_df[entities_df[\"entity_type\"] == \"institution\"].to_csv(data_folder + \"result/institution_patent_counts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents checkpoint\n",
    "patents.to_parquet(data_folder + \"/start/parquet/patents_cleaned_assignee_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents checkpoint load\n",
    "patents = pd.read_parquet(data_folder + \"/start/parquet/patents_cleaned_assignee_filtered.parquet\")\n",
    "f\"Num patents loaded: {patents.shape[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents of each assignee type\n",
    "patents[\"patent_assignee\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents['entity_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show counts of unique assignees and entity names by entity type\n",
    "print(\"Number of unique assignees by entity type:\")\n",
    "print(patents.groupby(\"entity_type\")[\"patent_assignee\"].nunique())\n",
    "print(\"\\nNumber of unique entity names by entity type:\")\n",
    "print(patents.groupby(\"entity_type\")[\"entity_name\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents[['entity_name', 'patent_assignee', 'patent_id']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "institutions_to_search = patents[\"entity_name\"].unique().tolist()\n",
    "data = []\n",
    "\n",
    "for entity_name in tqdm(institutions_to_search):\n",
    "    institution_matches = Institutions().search(entity_name.lower()).get()\n",
    "    for match in institution_matches:\n",
    "        data.append({\n",
    "            'entity_name': entity_name,\n",
    "            'institution_display_name': match['display_name'],\n",
    "            'institution_id': match['id']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sort_values('entity_name')\n",
    "\n",
    "df.to_csv(data_folder + \"start/csv/entities_institution_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + \"start/csv/entities_institution_ids.csv\", \"r\") as f:\n",
    "    entities_institution_ids = pd.read_csv(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patent Inventors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventors = dd.read_parquet(data_folder + \"start/dask/inventors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventors = inventors.merge(patents[['patent_id']], on='patent_id', how='inner').compute()\n",
    "f\"Num inventors associated with our patents: {inventors.shape[0]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventors[\"inventor_full_name\"] = inventors[\"inventor_first_name\"] + \" \" + inventors[\"inventor_last_name\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patent References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download your own version of marx patent data from the original source\n",
    "marx_references = dd.read_parquet(data_folder + \"start/marx_patent_references.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents[\"patent_id_us\"] = \"US-\" + patents.patent_id.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_references = marx_references.merge(patents, how=\"inner\", left_index=True, right_on=\"patent_id_us\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_references = patent_references[['patent_id', 'doi', 'patent_id_us']].drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Works (papers)\n",
    "We will fetch works (papers) from openalex based on candidancy. All papers by authors with a matching mame will be considered as potential matches."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_inventor_name(inventor_name: str) -> str:\n",
    "    if type(inventor_name) == str and \",\" in inventor_name:\n",
    "        inventor_name = inventor_name.split(\",\")[0]\n",
    "    return inventor_name\n",
    "\n",
    "inventor_list = [handle_inventor_name(inventor_name) for inventor_name in inventors[\"inventor_full_name\"].unique().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out non-string and empty\n",
    "inventor_list = [inventor for inventor in inventor_list if type(inventor) == str and inventor]\n",
    "f\"Number of unique inventor names: {len(inventor_list)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = openalex_db.get_candidates(inventor_list)\n",
    "candidates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only candidates with an institution in our entity list\n",
    "valid_institution_ids = set(entities_institution_ids[\"institution_id\"].to_list())\n",
    "def does_candidate_have_institution(row: pd.Series) -> bool:\n",
    "    candidate_institution_ids = json.loads(row[\"institution_ids\"])\n",
    "    return any(institution_id in valid_institution_ids for institution_id in candidate_institution_ids)\n",
    "candidate_has_institution = candidates.apply(does_candidate_have_institution, axis=1)\n",
    "candidates_filtered = candidates[candidate_has_institution]\n",
    "candidates_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = candidates_filtered"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work candidates\n",
    "\n",
    "Based on the filtered author candidate list we now pull candidates' works from openalex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = candidates[\"author_id\"].unique().tolist()\n",
    "len(author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_list = sorted(author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works, authorships = openalex_db.get_works_by_author_ids(\n",
    "    author_list,\n",
    "    limit_author_position=[\"first\", \"last\"],\n",
    "    work_min_date=\"2000-01-01\",\n",
    "    work_max_date=\"2022-12-31\",\n",
    ")\n",
    "f\"Got {len(works)} works and {len(authorships)} authorships\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = works.rename(columns={\n",
    "    \"doi\": \"work_doi\",\n",
    "    \"title\": \"work_title\",\n",
    "    \"abstract\": \"work_abstract\",\n",
    "    \"publication_date\": \"work_publication_date\",\n",
    "    \"referenced_works\": \"work_referenced_works\",\n",
    "    \"related_works\": \"work_related_works\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works.shape, authorships.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "works['work_related_works'] = works['work_related_works'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "works['work_referenced_works'] = works['work_referenced_works'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = works.dropna(subset=[\"work_title\"]).drop_duplicates(subset=[\"work_id\"])\n",
    "works.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorships['institutions'] = authorships['institutions'].map(lambda x: json.loads(x) if type(x) == str else [])\n",
    "authorships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only want to keep works where at least one author has an institution in our entity list\n",
    "# Filter out those work_ids where the author's institution doesn't belong to the list, using ids\n",
    "authorships_with_valid_institution_ids = authorships[authorships['institutions'].map(lambda x: any(institution_id in valid_institution_ids for institution_id in x))]\n",
    "authorships_with_valid_institution_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = works[works['work_id'].isin(authorships_with_valid_institution_ids['work_id'].unique())]\n",
    "works.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of these works have the word review in the title\n",
    "works[works['work_title'].str.contains(\"review\", na=False)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorships = authorships[authorships['work_id'].isin(works['work_id'].unique())]\n",
    "authorships.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a mapping of related works for reproducibility and speed purposes when re-running the code\n",
    "# Use existing mapping when available\n",
    "# Comment this out if the file doesn't exist yet\n",
    "with open(data_folder + \"registry/related_works_mapping.json\", \"r\") as f:\n",
    "    related_works_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_related = works[['work_id', 'work_related_works']]\n",
    "works_related = works_related[works_related['work_related_works'].apply(lambda x: len(x) > 0)]\n",
    "works_related = works_related.explode(column='work_related_works').rename(columns={'work_related_works': 'work_related_work_id'}).drop_duplicates()\n",
    "f\"Num related works: {len(works_related)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a graphical distribution of the number of related works per work_id, using pandas plot\n",
    "works_related.groupby(\"work_id\").size().plot(kind=\"hist\", bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one random related work for each work in works_related\n",
    "# Set the seed for reproducibility\n",
    "works_related_random_sample = works_related.groupby(\"work_id\").apply(lambda x: x.sample(1, random_state=42)).set_index(\"work_id\").to_dict()[\"work_related_work_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_related = works[['work_id']]\n",
    "works_related['work_related_work_id'] = works_related['work_id'].apply(lambda x: related_works_mapping.get(x, None) if x in related_works_mapping else works_related_random_sample.get(x, None))\n",
    "works_related.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_ids_to_fetch = works_related['work_related_work_id'].dropna().unique().tolist()\n",
    "len(work_ids_to_fetch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_related_pulled, authorships_related = openalex_db.get_works_by_work_ids(work_ids_to_fetch)\n",
    "works_related_pulled.shape, authorships_related.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_related_pulled = works_related_pulled.rename(columns={\n",
    "    \"doi\": \"work_doi\",\n",
    "    \"title\": \"work_title\",\n",
    "    \"abstract\": \"work_abstract\",\n",
    "    \"publication_date\": \"work_publication_date\",\n",
    "    \"referenced_works\": \"work_referenced_works\",\n",
    "    \"related_works\": \"work_related_works\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_related = works_related.merge(works_related_pulled, left_on=\"work_related_work_id\", right_on=\"work_id\", how=\"left\", suffixes=(\"\", \"_related\")).drop(columns=[\"work_id_related\"])\n",
    "print(\"Num pulled related works: \", len(works_related))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the related_works_mapping json file with the new works\n",
    "for work_id, related_work_id in works_related_random_sample.items():\n",
    "    if work_id not in related_works_mapping:\n",
    "        related_works_mapping[work_id] = related_work_id\n",
    "with open(data_folder + \"registry/related_works_mapping.json\", \"w\") as f:\n",
    "    json.dump(related_works_mapping, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching\n",
    "\n",
    "In this phase we will first generate valid patent-work pairs based on time range. Then, we fetch DOIs for all work references belonging to valid match pairs. Finally, we can compute the scores for each pair."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get paper - work pairs\n",
    "Match patents to publications (works), and also find an appropriate negative match by looking at related works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential Match merge route: Patent -> Inventor -> Candidate -> Work\n",
    "potential_matches = patents \\\n",
    "    .merge(right=inventors, on=\"patent_id\", how=\"inner\") \\\n",
    "    .merge(right=candidates, on=\"inventor_full_name\", how=\"inner\") \\\n",
    "    .merge(right=authorships[['author_id', 'work_id', 'author_position']], on=\"author_id\", how=\"inner\") \\\n",
    "    .merge(right=works, on=\"work_id\", how=\"inner\") \\\n",
    "    .sort_values([\"patent_id\", \"work_id\"]) \\\n",
    "    .drop_duplicates(subset=[\"patent_id\", \"work_id\"], keep=\"first\")\n",
    "potential_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches[\"relative_filing_date\"] = potential_matches.apply(\n",
    "    lambda row: (\n",
    "        row[\"work_publication_date\"].to_pydatetime().date() - convert_date(row[\"patent_filing_date\"])\n",
    "    ).days,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a boolean column to indicate whether the work is within the acceptable year range\n",
    "potential_matches[\"is_valid_range\"] = \\\n",
    "    (potential_matches[\"relative_filing_date\"] < 365 * 2) & \\\n",
    "    (potential_matches[\"relative_filing_date\"] > -365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many are valid?\n",
    "potential_matches[\"is_valid_range\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches = potential_matches[potential_matches[\"is_valid_range\"]]\n",
    "f\"Num valid potential matches: {len(potential_matches)}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull work reference dois from Openalex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches[[\"work_id\", \"work_referenced_works\"]].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow down works_related to only include works that are referenced by the potential_matches\n",
    "print(f\"Num potential matches: {len(potential_matches)}\")\n",
    "print(f\"Num works related: {len(works_related)}\")\n",
    "works_related = works_related[works_related[\"work_id\"].isin(potential_matches[\"work_id\"].unique())]\n",
    "print(f\"Num works related after narrowing down: {len(works_related)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_reference_ids = (\n",
    "    potential_matches[\"work_referenced_works\"].explode().unique().tolist()\n",
    ")\n",
    "related_work_reference_ids = (\n",
    "    works_related[\"work_referenced_works\"]\n",
    "    .map(lambda x: json.loads(x) if type(x) == str else [])\n",
    "    .explode()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "len(work_reference_ids), len(related_work_reference_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_reference_ids = list(filter(lambda x: x and type(x) == str and '/' in x, work_reference_ids))\n",
    "related_work_reference_ids = list(filter(lambda x: x and type(x) == str and '/' in x, related_work_reference_ids))\n",
    "len(work_reference_ids), len(related_work_reference_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_references = openalex_db.get_work_dois(work_reference_ids)\n",
    "related_works_references = openalex_db.get_work_dois(related_work_reference_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "works_references[\"doi\"] = works_references[\"doi\"].str[16:]\n",
    "related_works_references[\"doi\"] = related_works_references[\"doi\"].str[16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_id_to_doi = works_references[[\"work_id\", \"doi\"]].set_index(\"work_id\").to_dict()[\"doi\"]\n",
    "work_id_to_doi.update(\n",
    "    related_works_references[[\"work_id\", \"doi\"]].set_index(\"work_id\").to_dict()[\"doi\"]\n",
    ")\n",
    "print(len(work_id_to_doi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge negative related work information\n",
    "print(potential_matches.shape)\n",
    "potential_matches = potential_matches.merge(\n",
    "    right=works_related,\n",
    "    on=\"work_id\",\n",
    "    suffixes=(\"\", \"_negative\"),\n",
    "    how=\"left\",\n",
    ").drop_duplicates(subset=[\"patent_id\", \"work_id\"])\n",
    "potential_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches[\"referenced_works_dois\"] = potential_matches[\n",
    "    \"work_referenced_works\"\n",
    "].map(\n",
    "    lambda x: list(\n",
    "        filter(None, [work_id_to_doi.get(work_id) for work_id in x])\n",
    "        if type(x) == list\n",
    "        else []\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches[\"work_referenced_works_dois_negative\"] = potential_matches[\n",
    "    \"work_referenced_works_negative\"\n",
    "].map(\n",
    "    lambda x: list(\n",
    "        filter(None, [work_id_to_doi.get(work_id) for work_id in json.loads(x)])\n",
    "    )\n",
    "    if type(x) == str\n",
    "    else []\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches = potential_matches.merge(\n",
    "    right=patent_references.groupby(\"patent_id\")[\"doi\"]\n",
    "    .apply(list)\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    "    .compute(),\n",
    "    on=\"patent_id\",\n",
    "    how=\"left\",\n",
    ").rename(columns={\"doi\": \"patent_references_dois\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorships.rename(columns={\"full_name\": \"author_full_name\"}, inplace=True)\n",
    "authorships_related.rename(columns={\"full_name\": \"author_full_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add author list\n",
    "potential_matches = potential_matches.merge(\n",
    "    right=authorships[[\"work_id\", \"author_full_name\"]]\n",
    "    .groupby(\"work_id\")\n",
    "    .agg(list)\n",
    "    .rename(columns={\"author_full_name\": \"author_list\"}),\n",
    "    on=\"work_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Add negative author list\n",
    "potential_matches = potential_matches.merge(\n",
    "    right=authorships_related[[\"work_id\", \"author_full_name\"]]\n",
    "    .groupby(\"work_id\")\n",
    "    .agg(list)\n",
    "    .rename(columns={\"author_full_name\": \"author_list_negative\"}),\n",
    "    left_on=\"work_related_work_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Add inventor list\n",
    "potential_matches = potential_matches.merge(\n",
    "    right=inventors[[\"patent_id\", \"inventor_full_name\"]]\n",
    "    .groupby(\"patent_id\")\n",
    "    .agg(list)\n",
    "    .rename(columns={\"inventor_full_name\": \"inventor_list\"}),\n",
    "    on=\"patent_id\",\n",
    "    how=\"left\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.scores import inventor_overlap_score, doi_overlap_score, semantic_similarity_score_spacy, semantic_similarity_score_word_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches[\"inventor_score\"] = potential_matches.apply(\n",
    "    lambda row: inventor_overlap_score(row['inventor_list'], row['author_list']), axis=1,\n",
    ")\n",
    "potential_matches[\"inventor_score_negative\"] = potential_matches.apply(\n",
    "    lambda row: inventor_overlap_score(row[\"inventor_list\"], row[\"author_list_negative\"]), axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we remove those matches where the inventor score is 0\n",
    "# This situation probably means that openalex fetched some works where the author name doesn't exactly match\n",
    "potential_matches = potential_matches[potential_matches[\"inventor_score\"] > 0]\n",
    "potential_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches.loc[:,\"doi_overlap_score\"] = potential_matches.apply(\n",
    "    lambda row: doi_overlap_score(\n",
    "        row[\"patent_references_dois\"], row[\"referenced_works_dois\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "potential_matches.loc[:,\"doi_overlap_score_negative\"] = potential_matches.apply(\n",
    "    lambda row: doi_overlap_score(\n",
    "        row[\"patent_references_dois\"], row[\"work_referenced_works_dois_negative\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches.loc[:, [\n",
    "    \"patent_title\", \n",
    "    \"patent_abstract\", \n",
    "    \"work_title\", \n",
    "    \"work_abstract\", \n",
    "    \"work_title_negative\", \n",
    "    \"work_abstract_negative\"\n",
    "]] = \\\n",
    "    potential_matches[[\n",
    "    \"patent_title\", \n",
    "    \"patent_abstract\", \n",
    "    \"work_title\", \n",
    "    \"work_abstract\", \n",
    "    \"work_title_negative\", \n",
    "    \"work_abstract_negative\"\n",
    "]].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Calculating title semantic scores\")\n",
    "potential_matches.loc[:,\"title_semantic_score\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_spacy(row[\"patent_title\"], row[\"work_title\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Calculating title semantic scores negative\")\n",
    "potential_matches.loc[:,\"title_semantic_score_negative\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_spacy(row[\"patent_title\"], row[\"work_title_negative\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating abstract semantic scores\")\n",
    "potential_matches.loc[:,\"abstract_semantic_score\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_spacy(row[\"patent_abstract\"], row[\"work_abstract\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating abstract semantic scores negative\")\n",
    "potential_matches.loc[:,\"abstract_semantic_score_negative\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_spacy(row[\"patent_abstract\"], row[\"work_abstract_negative\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating semantic score titleabstract mean\")\n",
    "potential_matches.loc[:,\"semantic_score_titleabstract_mean\"] = potential_matches.apply(\n",
    "    lambda row: (row['title_semantic_score'] + row['abstract_semantic_score']) / 2,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating semantic score titleabstract mean negative\")\n",
    "potential_matches.loc[:,\"semantic_score_titleabstract_mean_negative\"] = potential_matches.apply(\n",
    "    lambda row: (row['title_semantic_score_negative'] + row['abstract_semantic_score_negative']) / 2,\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBERT scores\n",
    "print(f\"Calculating title semantic scores sbert\")\n",
    "potential_matches.loc[:,\"title_semantic_score_sbert\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_spacy(row[\"patent_title\"], row[\"work_title\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Calculating title semantic scores sbert negative\")\n",
    "potential_matches.loc[:,\"title_semantic_score_sbert_negative\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_spacy(row[\"patent_title\"], row[\"work_title_negative\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating abstract semantic scores sbert\")\n",
    "potential_matches.loc[:,\"abstract_semantic_score_sbert\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_spacy(row[\"patent_abstract\"], row[\"work_abstract\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating abstract semantic scores sbert negative\")\n",
    "potential_matches.loc[:,\"abstract_semantic_score_sbert_negative\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_spacy(row[\"patent_abstract\"], row[\"work_abstract_negative\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating semantic score titleabstract sbert mean\")\n",
    "potential_matches.loc[:,\"semantic_score_titleabstract_sbert_mean\"] = potential_matches.apply(\n",
    "    lambda row: (row['title_semantic_score_sbert'] + row['abstract_semantic_score_sbert']) / 2,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating semantic score titleabstract sbert mean negative\")\n",
    "potential_matches.loc[:,\"semantic_score_titleabstract_sbert_mean_negative\"] = potential_matches.apply(\n",
    "    lambda row: (row['title_semantic_score_sbert_negative'] + row['abstract_semantic_score_sbert_negative']) / 2,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Calculating title overlap score\")\n",
    "potential_matches[\"title_overlap_score\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_word_overlap(row[\"patent_title\"], row[\"work_title\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"Calculating title overlap score negative\")\n",
    "potential_matches[\"title_overlap_score_negative\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_word_overlap(row[\"patent_title\"], row[\"work_title_negative\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating abstract overlap score\")\n",
    "potential_matches[\"abstract_overlap_score\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_word_overlap(row[\"patent_abstract\"], row[\"work_abstract\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating abstract overlap score negative\")\n",
    "potential_matches[\"abstract_overlap_score_negative\"] = potential_matches.apply(\n",
    "    lambda row: semantic_similarity_score_word_overlap(row[\"patent_abstract\"], row[\"work_abstract_negative\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating overlap score titleabstract mean\")\n",
    "potential_matches[\"overlap_score_titleabstract_mean\"] = potential_matches.apply(\n",
    "    lambda row: (row['title_overlap_score'] + row['abstract_overlap_score']) / 2,\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(f\"Calculating overlap score titleabstract mean negative\")\n",
    "potential_matches[\"overlap_score_titleabstract_mean_negative\"] = potential_matches.apply(\n",
    "    lambda row: (row['title_overlap_score_negative'] + row['abstract_overlap_score_negative']) / 2,\n",
    "    axis=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the number of references for each patent\n",
    "potential_matches = potential_matches.merge(\n",
    "    right=patent_references.groupby(\"patent_id\").size().astype(int).rename(\"patent_num_references\").compute(),\n",
    "    how=\"left\",\n",
    "    left_on=\"patent_id\",\n",
    "    right_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the number of inventors for each patent\n",
    "potential_matches = potential_matches.merge(\n",
    "    right=inventors.groupby(\"patent_id\").size().astype(int).rename(\"patent_num_inventors\"),\n",
    "    how=\"left\",\n",
    "    left_on=\"patent_id\",\n",
    "    right_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the number of authors for each paper\n",
    "potential_matches = potential_matches.merge(\n",
    "    right=authorships.groupby(\"work_id\").size().astype(int).rename(\"paper_num_authors\"),\n",
    "    how=\"left\",\n",
    "    left_on=\"work_id\",\n",
    "    right_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the relative publication date for the negative matches\n",
    "def relative_filing_date_negative(row):\n",
    "    if not row[\"work_publication_date_negative\"]: return None\n",
    "    if type(row[\"work_publication_date_negative\"]) != str: return None\n",
    "    return (row[\"work_publication_date_negative\"].to_pydatetime().date() - convert_date(row[\"patent_filing_date\"])).days\n",
    "    \n",
    "potential_matches[\"relative_filing_date_negative\"] = potential_matches.apply(\n",
    "    relative_filing_date_negative,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = potential_matches.sort_values(by=[\"patent_id\", \"work_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results[\"relative_granted_date\"] = final_results.apply(\n",
    "    lambda row: (row[\"work_publication_date\"].to_pydatetime().date() - convert_date(row[\"patent_date\"])).days,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out matches where there is a paper with 'review' in the title\n",
    "print(f'Num matches before filtering review papers: {len(final_results)}')\n",
    "final_results = final_results[~(final_results['work_title'].str.contains('review', case=False))]\n",
    "print(f'Num matches after filtering review papers: {len(final_results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_parquet(data_folder + \"result/final_results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_excel(data_folder + \"result/final_results.xlsx\", engine=\"xlsxwriter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-project--Rqv9rzG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c108d40d9975865ff35cdd94384b4f8f866351b222c1141ec40a208496bb862"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
