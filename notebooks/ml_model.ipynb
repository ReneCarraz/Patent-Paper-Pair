{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, cross_validate, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    "from src.utils import convert_date\n",
    "\n",
    "data_folder = '/Path/To/Data/Folder/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = None # Load your labeled match data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"inventor_score\", \n",
    "    \"doi_overlap_score\", \n",
    "    \"overlap_score_titleabstract_mean\", \n",
    "    \"semantic_score_titleabstract_sbert_mean\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = matches[features].to_numpy()\n",
    "y = matches[\"match\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store precision, recall, F1 scores and accuracy for each fold\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "accuracies = []\n",
    "\n",
    "common_thresholds = np.linspace(0, 1, 500)\n",
    "\n",
    "for train_indices, test_indices in StratifiedKFold(n_splits=5, shuffle=True).split(x, y):\n",
    "    x_train, x_test = x[train_indices], x[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    y_pred_proba = lr.predict_proba(x_test)[:, 1]\n",
    "    y_pred = lr.predict(x_test)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
    "    f1_score = 2*(precision*recall)/(precision+recall)\n",
    "    \n",
    "    # Interpolate the precision, recall, and F1 score arrays to the common thresholds\n",
    "    precisions.append(np.interp(common_thresholds, thresholds, precision[:-1]))\n",
    "    recalls.append(np.interp(common_thresholds, thresholds, recall[:-1]))\n",
    "    f1_scores.append(np.interp(common_thresholds, thresholds, f1_score[:-1]))\n",
    "    \n",
    "    # Add accuracy score\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Convert lists to arrays for ease of calculation\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "f1_scores = np.array(f1_scores)\n",
    "accuracies = np.array(accuracies)\n",
    "\n",
    "# Calculate mean and standard deviation of precision, recall, f1 scores, and accuracy\n",
    "precision_mean = np.mean(precisions, axis=0)\n",
    "recall_mean = np.mean(recalls, axis=0)\n",
    "f1_scores_mean = np.mean(f1_scores, axis=0)\n",
    "accuracy_mean = np.mean(accuracies)\n",
    "\n",
    "precision_std = np.std(precisions, axis=0)\n",
    "recall_std = np.std(recalls, axis=0)\n",
    "f1_scores_std = np.std(f1_scores, axis=0)\n",
    "accuracy_std = np.std(accuracies)\n",
    "\n",
    "# Print average accuracy and its standard deviation\n",
    "print(\"Five-fold cross-validation results:\")\n",
    "print(f'Average Accuracy: {accuracy_mean:.2f} (+/- {accuracy_std:.2f})')\n",
    "\n",
    "# Print precision, recall and F1 score at threshold 0.5\n",
    "print(\"\\nScores at threshold 0.5:\")\n",
    "print(f'Precision: {precision_mean[250]:.4f} +/- {precision_std[250]:.4f}')\n",
    "print(f'Recall: {recall_mean[250]:.4f} +/- {recall_std[250]:.4f}')\n",
    "print(f'F1 Score: {f1_scores_mean[250]:.4f} +/- {f1_scores_std[250]:.4f}')\n",
    "\n",
    "\n",
    "# Print precision, recall and F1 score at threshold 0.6\n",
    "print(\"\\nScores at threshold 0.6:\")\n",
    "print(f'Precision: {precision_mean[300]:.4f} +/- {precision_std[300]:.4f}')\n",
    "print(f'Recall: {recall_mean[300]:.4f}' + f' +/- {recall_std[300]:.4f}')\n",
    "print(f'F1 Score: {f1_scores_mean[300]:.4f} +/- {f1_scores_std[300]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These are the coefficients\")\n",
    "print(features)\n",
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class Lissoni_2013_Model:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "        self.threshold = 0\n",
    "        \n",
    "    def fit(self, patent_documents: List[str], paper_documents: List[str]):\n",
    "        all_docs = patent_documents + paper_documents\n",
    "        self.vectorizer.fit(all_docs)\n",
    "        patent_vectors = self.vectorizer.transform(patent_documents).toarray()\n",
    "        paper_vectors = self.vectorizer.transform(paper_documents).toarray()\n",
    "        self.train_scores = [cosine_similarity(patent_vector.reshape(1, -1), paper_vector.reshape(1, -1))[0][0] for patent_vector, paper_vector in zip(patent_vectors, paper_vectors)]\n",
    "        \n",
    "    def predict(self, patent_doc, paper_doc, threshold_percentile=0.1):\n",
    "        threshold = np.percentile(self.train_scores, 100 - threshold_percentile * 100)\n",
    "        vector1 = self.vectorizer.transform([patent_doc]).toarray()\n",
    "        vector2 = self.vectorizer.transform([paper_doc]).toarray()\n",
    "        similarity = cosine_similarity(vector1, vector2)[0][0]\n",
    "        return similarity >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "class Magerman_2015_Model:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def predict(self, patent_doc, paper_doc, overlap_min_threshold=0.6, overlap_max_threshold=0.3):\n",
    "         # Tokenize the input string\n",
    "        patent_tokens = word_tokenize(patent_doc)\n",
    "        paper_tokens = word_tokenize(paper_doc)\n",
    "\n",
    "        # Remove stop words and Perform stemming\n",
    "        # processed_tokens = [stemmer.stem(w) for w in word_tokens if not w in stop_words]\n",
    "        patent_tokens = [self.stemmer.stem(w) for w in patent_tokens if not w in self.stop_words]\n",
    "        paper_tokens = [self.stemmer.stem(w) for w in paper_tokens if not w in self.stop_words]\n",
    "\n",
    "        # Calculate the number of common words\n",
    "        common_words = len(set(patent_tokens).intersection(set(paper_tokens)))\n",
    "\n",
    "        # Overlap over the number of words in the doc with the smaller number of words\n",
    "        overlap_minimum = common_words / min(len(patent_tokens), len(paper_tokens))\n",
    "        # Overlap over the number of words in the doc with the larger number of words\n",
    "        overlap_maximum = common_words / max(len(patent_tokens), len(paper_tokens))\n",
    "        return overlap_minimum >= overlap_min_threshold and overlap_maximum >= overlap_max_threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating precision, recall and F1 score for the different models\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "lissoni_model = Lissoni_2013_Model()\n",
    "magerman_model = Magerman_2015_Model()\n",
    "\n",
    "train_indices, test_indices = train_test_split(range(len(matches)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_patent_titles = matches.iloc[train_indices]['patent_title'].to_list()\n",
    "train_patent_abstracts = matches.iloc[train_indices]['patent_abstract'].to_list()\n",
    "train_paper_titles = matches.iloc[train_indices]['work_title'].to_list()\n",
    "train_paper_abstracts = matches.iloc[train_indices]['work_abstract'].to_list()\n",
    "\n",
    "# Concatenate titles and abstracts into \"documents\"\n",
    "train_patent_documents = [title + \" \" + abstract for title, abstract in zip(train_patent_titles, train_patent_abstracts)]\n",
    "train_paper_documents = [title + \" \" + abstract for title, abstract in zip(train_paper_titles, train_paper_abstracts)]\n",
    "\n",
    "lissoni_model.fit(train_patent_documents, train_paper_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lissoni_preds = []\n",
    "magerman_preds = []\n",
    "for test_index in test_indices:\n",
    "    patent_title = matches.iloc[test_index]['patent_title']\n",
    "    patent_abstract = matches.iloc[test_index]['patent_abstract']\n",
    "    patent_document = patent_title + \" \" + patent_abstract\n",
    "    paper_title = matches.iloc[test_index]['work_title']\n",
    "    paper_abstract = matches.iloc[test_index]['work_abstract']\n",
    "    paper_document = paper_title + \" \" + paper_abstract\n",
    "    \n",
    "    lissoni_preds.append(lissoni_model.predict(patent_document, paper_document))\n",
    "    magerman_preds.append(magerman_model.predict(patent_document, paper_document))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = matches[features].iloc[train_indices].to_numpy()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = matches[features].iloc[test_indices].to_numpy()\n",
    "x_test = scaler.transform(x_test)\n",
    "model_preds = LogisticRegression() \\\n",
    "    .fit(x_train, matches[\"match\"].iloc[train_indices].to_numpy()) \\\n",
    "    .predict(x_test).tolist()\n",
    "\n",
    "assert(len(lissoni_preds) == len(magerman_preds) == len(model_preds) == len(test_indices))\n",
    "\n",
    "print(\"Results of one 80%train 20%test split with comparison against Lissoni and Magerman models\")\n",
    "\n",
    "print(\"\\nLissoni Model\")\n",
    "print(\"Prediction percent: \", sum(lissoni_preds) / len(lissoni_preds))\n",
    "print(\"Precision, Recall, F1 Score: \", precision_recall_fscore_support(matches['match'].iloc[test_indices], lissoni_preds, average='binary'))\n",
    "\n",
    "print(\"\\nMagerman Model\")\n",
    "print(\"Prediction percent: \", sum(magerman_preds) / len(magerman_preds))\n",
    "print(\"Precision, Recall, F1 Score: \", precision_recall_fscore_support(matches['match'].iloc[test_indices], magerman_preds, average='binary'))\n",
    "\n",
    "print(\"\\nLinear Model\")\n",
    "print(\"Prediction percent: \", sum(model_preds) / len(model_preds))\n",
    "print(\"Precision, Recall, F1 Score: \", precision_recall_fscore_support(matches['match'].iloc[test_indices], model_preds, average='binary'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lissoni_preds = []\n",
    "magerman_preds = []\n",
    "for test_index in test_indices:\n",
    "    patent_title = matches.iloc[test_index]['patent_title']\n",
    "    patent_abstract = matches.iloc[test_index]['patent_abstract']\n",
    "    patent_document = patent_title + \" \" + patent_abstract\n",
    "    paper_title = matches.iloc[test_index]['work_title']\n",
    "    paper_abstract = matches.iloc[test_index]['work_abstract']\n",
    "    paper_document = paper_title + \" \" + paper_abstract\n",
    "    \n",
    "    lissoni_preds.append(lissoni_model.predict(patent_document, paper_document, threshold_percentile=0.45))\n",
    "    magerman_preds.append(magerman_model.predict(patent_document, paper_document, overlap_min_threshold=0.11, overlap_max_threshold=0.06))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = matches[features].iloc[train_indices].to_numpy()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = matches[features].iloc[test_indices].to_numpy()\n",
    "x_test = scaler.transform(x_test)\n",
    "model_preds = LogisticRegression() \\\n",
    "    .fit(x_train, matches[\"match\"].iloc[train_indices].to_numpy()) \\\n",
    "    .predict(x_test).tolist()\n",
    "\n",
    "assert(len(lissoni_preds) == len(magerman_preds) == len(model_preds) == len(test_indices))\n",
    "\n",
    "print(\"Results of one 80%train 20%test split with comparison against Lissoni and Magerman models\")\n",
    "print(\"Here, we adjusted the Lissoni and Magerman thresholds to match the prediction rate of the linear model.\")\n",
    "\n",
    "print(\"Lissoni Model\")\n",
    "print(\"Prediction percent: \", sum(lissoni_preds) / len(lissoni_preds))\n",
    "print(\"Precision, Recall, F1 Score: \", precision_recall_fscore_support(matches['match'].iloc[test_indices], lissoni_preds, average='binary'))\n",
    "print()\n",
    "\n",
    "print(\"Magerman Model\")\n",
    "print(\"Prediction percent: \", sum(magerman_preds) / len(magerman_preds))\n",
    "print(\"Precision, Recall, F1 Score: \", precision_recall_fscore_support(matches['match'].iloc[test_indices], magerman_preds, average='binary'))\n",
    "print()\n",
    "\n",
    "print(\"Linear Model\")\n",
    "print(\"Prediction percent: \", sum(model_preds) / len(model_preds))\n",
    "print(\"Precision, Recall, F1 Score: \", precision_recall_fscore_support(matches['match'].iloc[test_indices], model_preds, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"Order is: intercept, coef1 ,coef2, coef3, coef4\")\n",
    "\n",
    "def logit_p_value(model, x):\n",
    "   \n",
    "    p1 = model.predict_proba(x)\n",
    "    n1 = len(p1)\n",
    "    m1 = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    print(f'Coefficients: {coefs}')\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    answ = np.zeros((m1, m1))\n",
    "    for i in range(n1):\n",
    "        answ = answ + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p1[i,1] * p1[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(answ))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t1 =  coefs/se\n",
    "    p1 = (1 - norm.cdf(abs(t1))) * 2\n",
    "    return p1\n",
    "\n",
    "model = LogisticRegression().fit(x_train, y_train)\n",
    "values = logit_p_value(model, x_train)\n",
    "for value in values:\n",
    "    print('{:f}'.format(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_matches = pd.read_parquet(data_folder + 'result/final_results.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict all potential matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_potential_matches = potential_matches.dropna(subset=features)\n",
    "all_pred_probas = lr.predict_proba(scaler.transform(all_potential_matches[features].to_numpy()))\n",
    "\n",
    "# Threshold for positive matches\n",
    "threshold = 0.5\n",
    "\n",
    "all_preds = np.where(all_pred_probas[:, 1] > threshold, 1, 0)\n",
    "print(all_preds.shape)\n",
    "pd.Series(all_preds).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_potential_matches['model_classification'] = all_preds\n",
    "todays_date = datetime.datetime.now().date().strftime(\"%Y-%m-%d\")\n",
    "all_potential_matches[[\"patent_id\", \"work_id\", \"model_classification\"]] \\\n",
    "    .to_excel(data_folder + \"result/excels/final_results_model_classification_{todays_date}.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patent-project--Rqv9rzG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c108d40d9975865ff35cdd94384b4f8f866351b222c1141ec40a208496bb862"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
